{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YBv7EZdR7DS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dNmCPiHSx34"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "aG1pB63PS0To",
        "outputId": "f40519c5-a2cc-4e6a-cb0b-a4d480ee5f61"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ATGCCACAGCTAGATACATCCACCTGATTTATTATAATCTTTTCAA...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ATGAACGAAAATCTATTCGCTTCTTTCGCTGCCCCCTCAATAATAG...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ATGGAAACACCCTTCTACGGCGATGAGGCGCTGAGCGGCCTGGGCG...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ATGTGCACTAAAATGGAACAGCCCTTCTACCACGACGACTCATACG...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ATGAGCCGGCAGCTAAACAGAAGCCAGAACTGCTCCTTCAGTGACG...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sequence  class\n",
              "0  ATGCCACAGCTAGATACATCCACCTGATTTATTATAATCTTTTCAA...      4\n",
              "1  ATGAACGAAAATCTATTCGCTTCTTTCGCTGCCCCCTCAATAATAG...      4\n",
              "2  ATGGAAACACCCTTCTACGGCGATGAGGCGCTGAGCGGCCTGGGCG...      6\n",
              "3  ATGTGCACTAAAATGGAACAGCCCTTCTACCACGACGACTCATACG...      6\n",
              "4  ATGAGCCGGCAGCTAAACAGAAGCCAGAACTGCTCCTTCAGTGACG...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "human_dna = pd.read_table('human_data.txt')\n",
        "human_dna.head()\n",
        "\n",
        "chimp_dna = pd.read_table('chimp_data.txt')\n",
        "chimp_dna.head()\n",
        "\n",
        "dog_dna = pd.read_table('dog_data.txt')\n",
        "dog_dna.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXKpREHcS6HH"
      },
      "outputs": [],
      "source": [
        "#Let's define a function to collect all possible overlapping k-mers of a specified length from any sequence string.\n",
        "# function to convert sequence strings into k-mer words, default size = 6 (hexamer words)\n",
        "def Kmers_funct(seq, size=6):\n",
        "    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqWrGVDBTAyo"
      },
      "outputs": [],
      "source": [
        "human_dna['words'] = human_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\n",
        "human_dna = human_dna.drop('sequence', axis=1)\n",
        "chimp_dna['words'] = chimp_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\n",
        "chimp_dna = chimp_dna.drop('sequence', axis=1)\n",
        "dog_dna['words'] = dog_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\n",
        "dog_dna = dog_dna.drop('sequence', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR6HlTO7TDXY"
      },
      "outputs": [],
      "source": [
        "#Since we are going to use scikit-learn natural language processing tools to do the k-mer counting, we need to now convert the lists of k-mers for each gene into #string sentences of words that the count vectorizer can use. We can also make a y variable to hold the class labels.\n",
        "human_texts = list(human_dna['words'])\n",
        "for item in range(len(human_texts)):\n",
        "    human_texts[item] = ' '.join(human_texts[item])\n",
        "#separate labels\n",
        "y_human = human_dna.iloc[:, 0].values # y_human for human_dna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nkNxtS8VCz8"
      },
      "outputs": [],
      "source": [
        "#Now let's do the same for chimp and dog.\n",
        "chimp_texts = list(chimp_dna['words'])\n",
        "for item in range(len(chimp_texts)):\n",
        "    chimp_texts[item] = ' '.join(chimp_texts[item])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "LRpcHh2CxssR",
        "outputId": "ed663fc5-5f07-47a6-b973-e7984c3ead15"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'atgccc tgcccc gcccca ccccaa cccaac ccaact caacta aactaa actaaa ctaaat taaata aaatac aatacc ataccg taccgc accgcc ccgccg cgccgt gccgta ccgtat cgtatg gtatga tatgac atgacc tgaccc gaccca acccac cccacc ccacca caccat accata ccataa cataat ataatt taatta aattac attacc ttaccc tacccc accccc ccccca ccccat cccata ccatac catact atactc tactcc actcct ctcctg tcctga cctgac ctgaca tgacac gacact acacta cactat actatt ctattt tatttc atttct tttctc ttctcg tctcgt ctcgtc tcgtca cgtcac gtcacc tcaccc caccca acccaa cccaac ccaact caacta aactaa actaaa ctaaaa taaaaa aaaaat aaaata aaatat aatatt atatta tattaa attaaa ttaaat taaatt aaattc aattca attcaa ttcaaa tcaaat caaatt aaatta aattac attacc ttacca taccat accatc ccatct catcta atctac tctacc ctaccc tacccc accccc cccccc ccccct cccctc ccctca cctcac ctcacc tcacca caccaa accaaa ccaaaa caaaac aaaacc aaaccc aaccca acccat cccata ccataa cataaa ataaaa taaaaa aaaaat aaaata aaataa aataaa ataaaa taaaaa aaaaaa aaaaac aaaact aaacta aactac actaca ctacaa tacaat acaata caataa aataaa ataaac taaacc aaaccc aaccct accctg ccctga cctgag ctgaga tgagaa gagaac agaacc gaacca aaccaa accaaa ccaaaa caaaat aaaatg aaatga aatgaa atgaac tgaacg gaacga aacgaa acgaaa cgaaaa gaaaat aaaatc aaatct aatcta atctat tctatt ctattc tattcg attcgc ttcgct tcgctt cgcttc gcttca cttcat ttcatt tcattc cattcg attcgc ttcgct tcgctg cgctgc gctgcc ctgccc tgcccc gccccc ccccca ccccac cccaca ccacaa cacaat acaatc caatcc aatcct atccta tcctag'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chimp_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2GJ1wCUVJgj"
      },
      "outputs": [],
      "source": [
        "#separate labels\n",
        "y_chim = chimp_dna.iloc[:, 0].values # y_chim for chimp_dna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfqdK_lxVMiL"
      },
      "outputs": [],
      "source": [
        "dog_texts = list(dog_dna['words'])\n",
        "for item in range(len(dog_texts)):\n",
        "    dog_texts[item] = ' '.join(dog_texts[item])\n",
        "#separate labels\n",
        "y_dog = dog_dna.iloc[:, 0].values  # y_dog for dog_dna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8aym1FBV25K"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(human_texts)\n",
        "encoded_docs = tokenizer.texts_to_sequences(human_texts)\n",
        "max_length = max([len(s.split()) for s in human_texts])\n",
        "X = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y_human,\n",
        "                                                 test_size=0.20,random_state=42)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jGD0NQwWyQx"
      },
      "outputs": [],
      "source": [
        "Y_train = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blIFFb5NX7BI"
      },
      "outputs": [],
      "source": [
        "Y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDRNkZ9zWJGT",
        "outputId": "d5d519b5-7289-4b94-d056-a9db51e61fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 10)          44700     \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 40)               4960      \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7)                 287       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,947\n",
            "Trainable params: 49,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build neural network architecture\n",
        "\n",
        "adam = Adam(learning_rate=0.005)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10))\n",
        "model.add((LSTM(20)))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf0KnIR3XjER",
        "outputId": "0848e2f0-b5fd-4458-8f5a-904c543d044d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "18/18 [==============================] - 1273s 70s/step - loss: 1.8413 - accuracy: 0.2897 - val_loss: 1.7788 - val_accuracy: 0.3025\n",
            "Epoch 2/5\n",
            "18/18 [==============================] - 1215s 68s/step - loss: 1.6189 - accuracy: 0.3687 - val_loss: 1.4981 - val_accuracy: 0.4909\n",
            "Epoch 3/5\n",
            "16/18 [=========================>....] - ETA: 2:15 - loss: 1.3863 - accuracy: 0.5756"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(X_train,\n",
        "                    Y_train, \n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    verbose=1,\n",
        "                    batch_size=200,\n",
        "                    epochs=5,\n",
        "                     \n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r30Nt7vlw3lL"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, Y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFOaa-AezB84"
      },
      "outputs": [],
      "source": [
        "# graph representation of accuracy and loss of train and test data \n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'y', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel(\"No. of epochs\")\n",
        "plt.ylabel(\"percentage\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'g', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'y', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel(\"No. of epochs\")\n",
        "plt.ylabel(\"percentage\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdUc7HVoxmyZ"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "             chimp_texts[0],\n",
        "             chimp_texts[1],\n",
        "             chimp_texts[2],\n",
        "             chimp_texts[3]\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  sentence = tokenizer.texts_to_sequences([sentence])\n",
        "  sentence = pad_sequences(sentence,maxlen=256, truncating='pre')\n",
        "  result = np.argmax(model.predict(sentence),axis=-1)[0]\n",
        "  proba = np.max(model.predict(sentence))\n",
        "  print(f\"{result}: {proba}\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "LSTM Genome_classification project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}